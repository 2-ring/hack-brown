# DropCal

Converts messy text, images, audio, PDFs, and emails into calendar events via a 3-stage AI pipeline. Three-app monorepo: `backend/` (Flask/Python), `frontend/` (React/Vite/TypeScript), `mobile/` (Expo/React Native).

Production: dropcal.ai (frontend) / api.dropcal.ai (backend)

## Architecture

### Pipeline

The core is a 3-stage pipeline: one LLM call for extraction, deterministic date resolution, then optional personalization. Stages inherit from `BaseAgent` (`backend/pipeline/base_agent.py`), use `.with_structured_output(PydanticModel)` for typed returns, and load prompts from a `prompts/` directory relative to their module.

All pipeline code lives under `backend/pipeline/`:

```
pipeline/
├── input/              # Input preprocessing (audio, image, pdf, text, email)
├── extraction/         # EXTRACT stage (identify, consolidate, structure)
├── resolution/         # RESOLVE stage (Duckling temporal parsing)
├── personalization/    # PERSONALIZE stage (patterns, similarity, corrections)
├── modification/       # MODIFY (user edits, separate from main pipeline)
├── orchestrator.py     # SessionProcessor — runs the pipeline
├── models.py           # All Pydantic pipeline models
├── stream.py           # SSE stream management
├── events.py           # EventService (pipeline output CRUD)
└── session_routes.py   # SSE streaming routes
```

```
EXTRACT (1 LLM call) → RESOLVE (0 LLM, Duckling) → PERSONALIZE (0-1 LLM)

MODIFY ← user edits (separate from pipeline)
```

| Stage | File | Input → Output | Job |
|-------|------|---------------|-----|
| EXTRACT | `pipeline/extraction/extract.py` | raw text/image → `ExtractedEventBatch` (NL temporal) | Find all events, dedup, extract structured facts in a single LLM call |
| RESOLVE | `pipeline/resolution/temporal_resolver.py` | `ExtractedEvent` → `CalendarEvent` (ISO 8601) | Deterministic date resolution via Duckling, EXDATE generation |
| PERSONALIZE | `pipeline/personalization/agent.py` | `CalendarEvent` + patterns → personalized `CalendarEvent` | Apply learned preferences (calendar routing, title formatting) |
| MODIFY | `pipeline/modification/agent.py` | user edit request → modified event | Handle user corrections (not part of main pipeline) |

All Pydantic models live in `backend/pipeline/models.py`. EXTRACT processes everything in a single LLM call, RESOLVE runs per-event (deterministic), PERSONALIZE runs as a single batched LLM call if the user has enough history for personalization patterns.

### LLM Provider Config

`backend/config/text.py` controls which LLM each pipeline stage uses. Switch between presets by changing the `CONFIG` line:
- `TextModelConfig.all_grok()` — current default, uses $2.5k xAI credits
- `TextModelConfig.all_claude()` — production quality (claude-sonnet-4-5)
- `TextModelConfig.hybrid_optimized()` — Claude for complex tasks, Grok for simple

Each stage can use a different provider. `create_text_model(component)` returns the right LangChain LLM.

### Processing Flow

1. Frontend creates a session (`POST /api/sessions`) → backend returns session ID
2. `SessionProcessor` runs the pipeline asynchronously (title generated by the EXTRACT call, icon selection in parallel background thread)
3. Frontend polls `GET /api/sessions/:id` until status is `processed` or `error`
4. Uses standard formatting for guests or users with <10 events, personalized formatting otherwise

### Factory Patterns

- **Input processors** (`backend/pipeline/input/factory.py`): `InputProcessorFactory` routes to `AudioProcessor`, `ImageProcessor`, `PDFProcessor`, `TextFileProcessor`. All inherit `BaseInputProcessor` with `process()` and `supports_file()`.
- **Calendar providers** (`backend/calendars/factory.py`): Routes to `google/`, `microsoft/`, `apple/` subdirectories. Each has `auth`, `fetch`, `create` modules. Auto-routes to user's `primary_calendar_provider`.

## Database

Supabase PostgreSQL. Singleton client via `get_supabase()` in `database/supabase_client.py`.

Three tables: `users`, `sessions`, `events`. Models in `backend/database/models.py` use static methods for CRUD (not an ORM).

Key patterns:
- OAuth tokens are **encrypted with Fernet** (`utils/encryption.py`). Never store tokens in plaintext.
- Session statuses: `pending` → `processing` → `processed` | `error`
- Guest sessions use `secrets.token_hex(32)` access tokens, separate endpoints (`/api/sessions/guest`)
- Events support soft delete (`deleted_at`), draft status, correction history, and 384-dim vector embeddings (pgvector)
- Conflicts checked via `get_conflicting_events` Supabase RPC function

## Auth

- Supabase Auth handles JWT tokens. Backend validates with `@require_auth` decorator (`auth/middleware.py`) which sets `request.user_id`.
- Frontend: `AuthContext` (React Context) manages state, `useAuth()` hook for components.
- Google OAuth for auth + calendar scopes. Microsoft MSAL for Outlook. Apple CalDAV with app-specific passwords.

## Key Commands

```bash
# Backend
docker-compose up -d duckling                  # Start Duckling (required for temporal parsing)
cd backend && python app.py                    # Dev server (port 5000)
cd backend && gunicorn wsgi:app -b 0.0.0.0:8000  # Production
cd backend && pytest tests/                    # Run tests

# Frontend
cd frontend && npm run dev                     # Vite dev server (port 5173)
cd frontend && npm run build                   # Production build
cd frontend && npx tsc --noEmit                # Type check

# Deploy (CI/CD does this on push to main)
cd backend && eb deploy dropcal-prod           # Backend → Elastic Beanstalk
cd frontend && aws s3 sync dist/ s3://dropcal-frontend --delete  # Frontend → S3

# Database
supabase db push                               # Apply migrations
supabase migration new <name>                  # Create migration
```

## Conventions

- **Python**: PascalCase classes, snake_case functions, Pydantic models for all structured LLM output. Pipeline stages use `ChatPromptTemplate` → `chain.invoke()` or Instructor. Flask blueprints for route groups.
- **TypeScript**: PascalCase components, camelCase functions/hooks. API client in `frontend/src/api/backend-client.ts`. Types in `frontend/src/api/types.ts`.
- **New pipeline stages**: Inherit `BaseAgent`, implement `execute()`, add Pydantic model to `pipeline/models.py`, put prompt in `prompts/` dir next to the stage's module.
- **New calendar providers**: Add `auth.py`, `fetch.py`, `create.py` in `backend/calendars/<provider>/`, register in `factory.py`.
- **New input processors**: Inherit `BaseInputProcessor`, register in `app.py` via `input_processor_factory.register_processor()`.

## Analytics & LLM Observability (PostHog)

PostHog tracks LLM costs, latency, token usage, and product analytics. Project ID: 308768. Dashboard: https://us.posthog.com/project/308768

**How it works:** `config/posthog.py` initializes a PostHog client at startup. Before each pipeline run, `set_tracking_context(distinct_id, trace_id)` sets the user/trace for the current thread. Every `chain.invoke()` call passes `config=get_invoke_config()` which attaches a PostHog LangChain `CallbackHandler` that automatically captures model, tokens, cost, and latency.

**Backend (LLM observability):**
- `backend/config/posthog.py` — client init, thread-local tracking context, `get_invoke_config()` helper
- Callbacks attached in: `pipeline/extraction/extract.py`, `pipeline/modification/agent.py`, `pipeline/personalization/agent.py`, `pipeline/personalization/pattern_discovery.py`

**Frontend (product analytics):**
- `frontend/src/main.tsx` — PostHog init + `PostHogProvider` wrapper. Autocapture, pageviews, and pageleave enabled.
- `frontend/src/auth/AuthContext.tsx` — `posthog.identify()` on sign-in/session restore, `posthog.reset()` on sign-out. Links frontend events to backend LLM costs by shared user ID.

**Env vars:**
- Backend: `POSTHOG_API_KEY`, `POSTHOG_HOST`, `POSTHOG_PERSONAL_API_KEY`, `POSTHOG_PROJECT_ID`
- Frontend: `VITE_POSTHOG_KEY`, `VITE_POSTHOG_HOST`

**PostHog API:** Use the personal API key for querying analytics programmatically:
```bash
curl -H "Authorization: Bearer $POSTHOG_PERSONAL_API_KEY" \
  "https://us.posthog.com/api/projects/$POSTHOG_PROJECT_ID/insights/"
```

## Gotchas

- `app.py` initializes ALL pipeline stages and services at module import time — heavy startup, but needed for Gunicorn preloading.
- Rate limiting uses Redis in production (`REDIS_URL`), falls back to in-memory in dev.
- Max file upload: 25MB (`app.config['MAX_CONTENT_LENGTH']`).
- Backend `sys.path` manipulation at top of `app.py` — imports assume `backend/` is the working directory.
- CORS allows `localhost:3000`, `localhost:5173`, `dropcal.ai`, `www.dropcal.ai` only.

## Rules

- Only create markdown files for genuinely useful information. No completion reports, summaries, or self-evident docs.
- AWS CLI (`aws`) and Supabase CLI (`supabase`) are installed and authenticated. Use them directly.
- PostHog personal API key (`POSTHOG_PERSONAL_API_KEY`) is set in `backend/.env`. Use it to query the PostHog API directly (events, insights, dashboards, feature flags, experiments) as needed.
- Don't modify `utils/encryption.py` or token encryption logic without explicit request.
- When adding new endpoints, follow the blueprint pattern (see `auth/routes.py`, `calendars/routes.py`, `pipeline/session_routes.py`).
